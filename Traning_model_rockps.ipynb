{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Traning_model_rockps.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNNaVxIbgRwL+sx7qEmcAQS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Me-RockingAbhi/My-Projects/blob/master/Traning_model_rockps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NClSdhp0EoVv",
        "colab_type": "code",
        "outputId": "b5cbecdf-106e-4283-8a2f-9c79e28e5fb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBjfF6r9I2xx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" In my image folder of rock namings are alphanumeric ie,1rock,2rock.....nrock \n",
        "     so i have to split the name on 'r' and then join again it with 'r'+name\n",
        "     similirly all folder like scissors,paper and none are of same name and at different location therefore\n",
        "      i have to split and join name\n",
        "                                                                                \"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from keras.preprocessing import image\n",
        "from pathlib import Path\n",
        "path=Path(\"/content/gdrive/My Drive/rock\")\n",
        "img_name=[]\n",
        "img_array=[]\n",
        "dir=path.glob(\"*\")\n",
        "for i in dir:\n",
        "  name=str(i).split(\"r\")[-1]\n",
        "  name=name.split(\".\")[0];\n",
        "  pic=image.load_img(i,target_size=(224,224))\n",
        "  pic_arr=image.img_to_array(pic)\n",
        "  img_name.append(\"r\"+name)\n",
        "  print(name)\n",
        "  img_array.append(pic_arr)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMu5xmSYRPiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path=Path(\"/content/gdrive/My Drive/scissors\")\n",
        "dir=path.glob(\"*\")\n",
        "for i in dir:\n",
        "  name=str(i).split(\"s\")[-1]\n",
        "  name=name.split(\".\")[0]\n",
        "  pic=image.load_img(i,target_size=(224,224))\n",
        "  pic_arr=image.img_to_array(pic)\n",
        "  img_name.append(\"s\"+name)\n",
        "  img_array.append(pic_arr)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz2fh2E5TAxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path=Path(\"/content/gdrive/My Drive/Paper\")\n",
        "dir=path.glob(\"*\")\n",
        "for i in dir:\n",
        "  name=str(i).split(\"p\")[-1]\n",
        "  name=name.split(\".\")[0]\n",
        "  pic=image.load_img(i,target_size=(224,224))\n",
        "  pic_arr=image.img_to_array(pic)\n",
        "  img_name.append(\"p\"+name)\n",
        "  img_array.append(pic_arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxBS9F4-TPUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path=Path(\"/content/gdrive/My Drive/Image/none\")\n",
        "dir=path.glob(\"*\")\n",
        "for i in dir:\n",
        "  name=str(i).split(\"n\")[-1]\n",
        "  name=name.split(\".\")[0]\n",
        "  pic=image.load_img(i,target_size=(224,224))\n",
        "  pic_arr=image.img_to_array(pic)\n",
        "  img_name.append(\"n\"+name)\n",
        "  img_array.append(pic_arr)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMH46yHFT0qL",
        "colab_type": "code",
        "outputId": "c5373bda-d247-4b3c-9255-e95d0c8ddee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(img_name),len(img_array))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "953 953\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA_zcpRpYGnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" I have used densenet169 as it is smaller in size and also have good aacuracy\"\"\"\n",
        "from keras.applications import DenseNet169\n",
        "from keras.layers import  Convolution2D,GlobalAveragePooling2D,Dense,Dropout,Activation\n",
        "from keras import models\n",
        "from keras.models import Sequential\n",
        "model=Sequential([DenseNet169(input_shape=(224,224,3),include_top=False),\n",
        "                  Dropout(0.3),\n",
        "                   Convolution2D(4, (3, 3), padding='valid'),\n",
        "                  Activation('relu'),\n",
        "                  GlobalAveragePooling2D(),\n",
        "                  Activation('softmax')])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dws0KTLqZ4jE",
        "colab_type": "code",
        "outputId": "cad87012-c935-4756-89f5-e3b6619173c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "\"\"\" My Gpu is very slow therefore its taking very much time for a single epochs,i have alredy run 3-4 epochs earlier,\n",
        " before it was 12 but i think it will take very much time therefore i'm making it as 1\"\"\"\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le=LabelEncoder()\n",
        "img_name=le.fit_transform(img_name)\n",
        "y=np_utils.to_categorical(img_name)\n",
        "model.compile(optimizer=Adam(lr=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "model.fit(np.array(img_array),y,verbose=1,epochs=1)\n",
        "model.save(\"r-p-s.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "953/953 [==============================] - 983s 1s/step - loss: 0.3108 - accuracy: 0.9706\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}